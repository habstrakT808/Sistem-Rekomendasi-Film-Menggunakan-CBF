# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Film - Hafiyan Al Muqaffi Umary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yCiH1qFeE34XbafIcBFFORqAIQcw40xZ

# **SISTEM REKOMENDASI FILM**

## Menggunakan pendekatan Content-based Filtering dan Collaborative Filtering

## Persiapan Dataset dan Library

### Import library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import io
import requests
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import re
import os
import warnings
warnings.filterwarnings('ignore')

# Mengatur style visualisasi
plt.style.use('fivethirtyeight')
sns.set_style('whitegrid')

"""### Fungsi untuk mengunduh dan mengekstrak dataset"""

def download_and_extract_data():
    """
    Mengunduh dataset MovieLens 100K dan mengekstraknya
    """
    # URL dataset
    url = "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip"

    print("Mengunduh dataset...")
    r = requests.get(url)

    # Menyimpan file zip
    with open("ml-latest-small.zip", "wb") as file:
        file.write(r.content)

    # Mengekstrak file
    print("Mengekstrak file...")
    with ZipFile("ml-latest-small.zip", "r") as zip_ref:
        zip_ref.extractall()

    print("Dataset berhasil diunduh dan diekstrak!")

"""### Fungsi untuk memuat dataset"""

def load_data():
    """
    Memuat dataset MovieLens
    """
    # Jika dataset belum ada, unduh terlebih dahulu
    if not os.path.exists("ml-latest-small"):
        download_and_extract_data()

    # Memuat data rating
    ratings = pd.read_csv('ml-latest-small/ratings.csv')

    # Memuat data film
    movies = pd.read_csv('ml-latest-small/movies.csv')

    # Memuat data tags (jika diperlukan)
    tags = pd.read_csv('ml-latest-small/tags.csv')

    # Memuat data links (jika diperlukan)
    links = pd.read_csv('ml-latest-small/links.csv')

    return ratings, movies, tags, links

"""### Memuat dataset"""

ratings, movies, tags, links = load_data()

# Melihat 5 data pertama dari setiap dataset
print("Data ratings:")
print(ratings.head())
print("\nData movies:")
print(movies.head())
print("\nData tags:")
print(tags.head())
print("\nData links:")
print(links.head())

# Memeriksa informasi dataset
print("\nInformasi dataset ratings:")
print(ratings.info())
print("\nInformasi dataset movies:")
print(movies.info())

# Memeriksa statistik deskriptif
print("\nStatistik deskriptif ratings:")
print(ratings.describe())

"""## DATA UNDERSTANDING"""

# Memeriksa jumlah data di setiap dataset
print(f"Jumlah data ratings: {len(ratings)}")
print(f"Jumlah film unik: {len(movies)}")
print(f"Jumlah pengguna unik: {ratings['userId'].nunique()}")

# Visualisasi distribusi rating
plt.figure(figsize=(10, 6))
sns.countplot(x='rating', data=ratings)
plt.title('Distribusi Rating Film')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.savefig('rating_distribution.png')
plt.close()

# Visualisasi jumlah rating per film
movie_ratings_count = ratings.groupby('movieId').size().reset_index(name='count')
movie_ratings_count = movie_ratings_count.sort_values('count', ascending=False)

plt.figure(figsize=(12, 6))
sns.histplot(movie_ratings_count['count'], bins=50, kde=True)
plt.title('Distribusi Jumlah Rating per Film')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah Film')
plt.xscale('log')
plt.savefig('ratings_per_movie.png')
plt.close()

# Visualisasi jumlah rating per pengguna
user_ratings_count = ratings.groupby('userId').size().reset_index(name='count')
user_ratings_count = user_ratings_count.sort_values('count', ascending=False)

plt.figure(figsize=(12, 6))
sns.histplot(user_ratings_count['count'], bins=50, kde=True)
plt.title('Distribusi Jumlah Rating per Pengguna')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah Pengguna')
plt.savefig('ratings_per_user.png')
plt.close()

"""### Analisis genre film

"""

# Ekstrak semua genre yang ada
all_genres = []
for genres in movies['genres'].str.split('|'):
    all_genres.extend(genres)

unique_genres = pd.Series(all_genres).value_counts().reset_index()
unique_genres.columns = ['genre', 'count']

plt.figure(figsize=(14, 8))
sns.barplot(x='genre', y='count', data=unique_genres.head(15))
plt.title('15 Genre Film Terpopuler')
plt.xlabel('Genre')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('top_genres.png')
plt.close()

# Menggabungkan data film dengan rating rata-rata
movie_avg_rating = ratings.groupby('movieId')['rating'].mean().reset_index()
movie_avg_rating.columns = ['movieId', 'avgRating']

movies_with_ratings = pd.merge(movies, movie_avg_rating, on='movieId', how='left')
movies_with_ratings['avgRating'] = movies_with_ratings['avgRating'].fillna(0)

# Visualisasi film dengan rating tertinggi (min 100 rating)
movie_rating_count = ratings.groupby('movieId')['rating'].count().reset_index()
movie_rating_count.columns = ['movieId', 'ratingCount']

movies_with_stats = pd.merge(movies_with_ratings, movie_rating_count, on='movieId', how='left')
movies_with_stats['ratingCount'] = movies_with_stats['ratingCount'].fillna(0)

popular_movies = movies_with_stats[movies_with_stats['ratingCount'] >= 100].sort_values('avgRating', ascending=False)

plt.figure(figsize=(14, 8))
sns.barplot(x='avgRating', y='title', data=popular_movies.head(15))
plt.title('15 Film dengan Rating Tertinggi (Min. 100 Rating)')
plt.xlabel('Rating Rata-rata')
plt.ylabel('Judul Film')
plt.tight_layout()
plt.savefig('top_rated_movies.png')
plt.close()

"""## DATA PREPARATION"""

# Menghapus film yang memiliki sedikit rating (kurang dari 5 rating)
movie_counts = ratings['movieId'].value_counts()
movies_to_keep = movie_counts[movie_counts >= 5].index

filtered_ratings = ratings[ratings['movieId'].isin(movies_to_keep)]
print(f"Jumlah rating sebelum filtering: {len(ratings)}")
print(f"Jumlah rating setelah filtering: {len(filtered_ratings)}")

# Menghapus pengguna yang memberikan sedikit rating (kurang dari 5 rating)
user_counts = filtered_ratings['userId'].value_counts()
users_to_keep = user_counts[user_counts >= 5].index

filtered_ratings = filtered_ratings[filtered_ratings['userId'].isin(users_to_keep)]
print(f"Jumlah rating setelah filtering pengguna: {len(filtered_ratings)}")

# Membuat kolom tahun dari judul film
def extract_year(title):
    match = re.search(r'\((\d{4})\)$', title)
    if match:
        return int(match.group(1))
    return None

movies['year'] = movies['title'].apply(extract_year)

"""### Preprocessing data untuk Content-Based Filtering"""

# Membuat fitur berdasarkan genre
movies['genres'] = movies['genres'].fillna('')

# Membuat TF-IDF matrix dari genre
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['genres'].str.replace('|', ' '))

# Menghitung cosine similarity antar film
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Membuat mapping dari indeks ke movieId dan sebaliknya
indices = pd.Series(movies.index, index=movies['movieId']).drop_duplicates()
movie_indices = pd.Series(movies['movieId'].values, index=movies.index)

"""### Preprocessing data untuk Collaborative Filtering"""

# Memisahkan data menjadi training dan testing
train_data, test_data = train_test_split(
    filtered_ratings,
    test_size=0.2,
    random_state=42
)

# Membuat mapping movieId dan userId ke indeks berurutan
user_ids = filtered_ratings['userId'].unique().tolist()
movie_ids = filtered_ratings['movieId'].unique().tolist()

user_to_index = {user_id: i for i, user_id in enumerate(user_ids)}
movie_to_index = {movie_id: i for i, movie_id in enumerate(movie_ids)}

# Konversi data ke format yang sesuai untuk model
def map_ids(row):
    return {
        'user': user_to_index[row['userId']],
        'movie': movie_to_index[row['movieId']],
        'rating': row['rating']
    }

train_mapped = train_data.apply(map_ids, axis=1).tolist()
test_mapped = test_data.apply(map_ids, axis=1).tolist()

"""## MODELING - CONTENT-BASED FILTERING

"""

def get_recommendations_content_based(movie_id, cosine_sim=cosine_sim, movies=movies, indices=indices, movie_indices=movie_indices, n=10):
    """
    Memberikan rekomendasi film berdasarkan kesamaan genre

    Parameters:
    -----------
    movie_id : int
        ID film yang menjadi dasar rekomendasi
    cosine_sim : numpy.ndarray
        Matrix kesamaan kosinus antar film
    movies : pandas.DataFrame
        DataFrame berisi data film
    indices : pandas.Series
        Mapping dari movieId ke indeks
    movie_indices : pandas.Series
        Mapping dari indeks ke movieId
    n : int
        Jumlah rekomendasi yang diinginkan

    Returns:
    --------
    pandas.DataFrame
        DataFrame berisi film yang direkomendasikan
    """
    # Mendapatkan indeks film dari movieId
    if movie_id not in indices:
        return pd.DataFrame()

    idx = indices[movie_id]

    # Mendapatkan skor kesamaan dengan semua film
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Mengurutkan film berdasarkan skor kesamaan
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Mendapatkan indeks film yang direkomendasikan (tidak termasuk film itu sendiri)
    sim_scores = sim_scores[1:n+1]
    movie_indices_rec = [i[0] for i in sim_scores]

    # Mendapatkan movieId dari indeks
    movie_ids_rec = [movie_indices[i] for i in movie_indices_rec]

    # Mengembalikan data film yang direkomendasikan
    return movies[movies['movieId'].isin(movie_ids_rec)][['movieId', 'title', 'genres']]

"""### Contoh penggunaan Content-Based Filtering"""

# Mencari film "Toy Story"
toy_story = movies[movies['title'].str.contains('Toy Story', case=False)].iloc[0]
print(f"Film dasar untuk rekomendasi: {toy_story['title']} (ID: {toy_story['movieId']})")

# Mendapatkan rekomendasi
content_based_recommendations = get_recommendations_content_based(toy_story['movieId'])
print("\nRekomendasi berdasarkan Content-Based Filtering:")
print(content_based_recommendations)

"""## MODELING - COLLABORATIVE FILTERING"""

# Mendefinisikan model Neural Collaborative Filtering
def create_ncf_model(num_users, num_movies, embedding_size=50):
    """
    Membuat model Neural Collaborative Filtering

    Parameters:
    -----------
    num_users : int
        Jumlah pengguna unik
    num_movies : int
        Jumlah film unik
    embedding_size : int
        Ukuran embedding vector

    Returns:
    --------
    tensorflow.keras.Model
        Model NCF yang telah didefinisikan
    """
    # Input layers
    user_input = keras.Input(shape=(1,), name='user_input')
    movie_input = keras.Input(shape=(1,), name='movie_input')

    # Embedding layers
    user_embedding = layers.Embedding(
        input_dim=num_users,
        output_dim=embedding_size,
        name='user_embedding'
    )(user_input)

    movie_embedding = layers.Embedding(
        input_dim=num_movies,
        output_dim=embedding_size,
        name='movie_embedding'
    )(movie_input)

    # Flatten embeddings
    user_vector = layers.Flatten()(user_embedding)
    movie_vector = layers.Flatten()(movie_embedding)

    # Concatenate embeddings
    concat = layers.Concatenate()([user_vector, movie_vector])

    # Dense layers
    dense1 = layers.Dense(128, activation='relu')(concat)
    dense1 = layers.Dropout(0.2)(dense1)
    dense2 = layers.Dense(64, activation='relu')(dense1)
    dense2 = layers.Dropout(0.2)(dense2)

    # Output layer
    output = layers.Dense(1)(dense2)

    # Create model
    model = keras.Model(inputs=[user_input, movie_input], outputs=output)

    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mean_squared_error'
    )

    return model

"""### Membuat dataset TensorFlow"""

def create_tf_dataset(data_mapped, batch_size=64, shuffle=True):
    """
    Membuat dataset TensorFlow dari data yang sudah dimapping

    Parameters:
    -----------
    data_mapped : list
        List berisi dictionary dengan keys 'user', 'movie', dan 'rating'
    batch_size : int
        Ukuran batch
    shuffle : bool
        Apakah data perlu diacak

    Returns:
    --------
    tf.data.Dataset
        Dataset TensorFlow
    """
    users = np.array([x['user'] for x in data_mapped])
    movies = np.array([x['movie'] for x in data_mapped])
    ratings = np.array([x['rating'] for x in data_mapped])

    dataset = tf.data.Dataset.from_tensor_slices(({
        'user_input': users,
        'movie_input': movies
    }, ratings))

    if shuffle:
        dataset = dataset.shuffle(buffer_size=len(data_mapped))

    dataset = dataset.batch(batch_size)

    return dataset

# Membuat dataset TensorFlow
train_dataset = create_tf_dataset(train_mapped)
test_dataset = create_tf_dataset(test_mapped, shuffle=False)

# Membuat dan melatih model
num_users = len(user_ids)
num_movies = len(movie_ids)

# Membuat model
model = create_ncf_model(num_users, num_movies)

# Mendefinisikan callbacks
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Melatih model
history = model.fit(
    train_dataset,
    epochs=20,
    validation_data=test_dataset,
    callbacks=[early_stopping]
)

# Visualisasi loss selama training
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('model_loss.png')
plt.close()

# Fungsi untuk mendapatkan rekomendasi dengan Collaborative Filtering
def get_recommendations_collaborative(user_id, movies_df=movies, ratings_df=filtered_ratings, model=model,
                                     user_to_index=user_to_index, movie_to_index=movie_to_index, n=10):
    """
    Memberikan rekomendasi film berdasarkan Collaborative Filtering

    Parameters:
    -----------
    user_id : int
        ID pengguna
    movies_df : pandas.DataFrame
        DataFrame berisi data film
    ratings_df : pandas.DataFrame
        DataFrame berisi data rating
    model : tensorflow.keras.Model
        Model Collaborative Filtering
    user_to_index : dict
        Mapping dari userId ke indeks
    movie_to_index : dict
        Mapping dari movieId ke indeks
    n : int
        Jumlah rekomendasi yang diinginkan

    Returns:
    --------
    pandas.DataFrame
        DataFrame berisi film yang direkomendasikan
    """
    # Mendapatkan film yang belum ditonton oleh pengguna
    user_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()
    movies_to_predict = movies_df[~movies_df['movieId'].isin(user_movies)]['movieId'].values

    # Jika pengguna tidak ada dalam dataset training
    if user_id not in user_to_index:
        # Kembalikan rekomendasi film populer
        top_movies = ratings_df.groupby('movieId')['rating'].mean().sort_values(ascending=False).index[:n]
        return movies_df[movies_df['movieId'].isin(top_movies)][['movieId', 'title', 'genres']]

    # Membuat data untuk prediksi
    user_idx = user_to_index[user_id]
    movie_indices = [movie_to_index[m] for m in movies_to_predict if m in movie_to_index]

    if not movie_indices:
        # Jika tidak ada film yang bisa diprediksi, kembalikan rekomendasi film populer
        top_movies = ratings_df.groupby('movieId')['rating'].mean().sort_values(ascending=False).index[:n]
        return movies_df[movies_df['movieId'].isin(top_movies)][['movieId', 'title', 'genres']]

    users = np.array([user_idx] * len(movie_indices))
    movies = np.array(movie_indices)

    # Prediksi rating
    predictions = model.predict({
        'user_input': users,
        'movie_input': movies
    })

    # Menggabungkan film dengan prediksi rating
    movie_ids = [list(movie_to_index.keys())[list(movie_to_index.values()).index(idx)] for idx in movies]
    predicted_ratings = predictions.flatten()

    # Membuat DataFrame prediksi
    pred_df = pd.DataFrame({
        'movieId': movie_ids,
        'predicted_rating': predicted_ratings
    })

    # Mengurutkan berdasarkan prediksi rating
    pred_df = pred_df.sort_values('predicted_rating', ascending=False)

    # Mendapatkan top-n rekomendasi
    top_movie_ids = pred_df.head(n)['movieId'].values

    # Mengembalikan data film yang direkomendasikan
    return movies_df[movies_df['movieId'].isin(top_movie_ids)][['movieId', 'title', 'genres']]

"""### Contoh penggunaan Collaborative Filtering"""

# Memilih pengguna
sample_user_id = filtered_ratings['userId'].value_counts().index[0]
print(f"\nRekomendasi untuk Pengguna ID: {sample_user_id}")

# Mendapatkan film yang sudah ditonton oleh pengguna
user_watched_movies = filtered_ratings[filtered_ratings['userId'] == sample_user_id]
user_watched_movies = pd.merge(user_watched_movies, movies[['movieId', 'title']], on='movieId')
print("\nBeberapa film yang sudah ditonton oleh pengguna:")
print(user_watched_movies[['title', 'rating']].sort_values('rating', ascending=False).head())

# Mendapatkan rekomendasi
collaborative_recommendations = get_recommendations_collaborative(sample_user_id)
print("\nRekomendasi berdasarkan Collaborative Filtering:")
print(collaborative_recommendations)

"""## EVALUATION

"""

# Evaluasi Content-Based Filtering
def evaluate_content_based(movies_df, ratings_df, indices, cosine_sim, movie_indices, k=10):
    """
    Evaluasi model Content-Based Filtering menggunakan precision@k

    Parameters:
    -----------
    movies_df : pandas.DataFrame
        DataFrame berisi data film
    ratings_df : pandas.DataFrame
        DataFrame berisi data rating
    indices : pandas.Series
        Mapping dari movieId ke indeks
    cosine_sim : numpy.ndarray
        Matrix kesamaan kosinus antar film
    movie_indices : pandas.Series
        Mapping dari indeks ke movieId
    k : int
        Jumlah rekomendasi yang dievaluasi

    Returns:
    --------
    float
        Nilai precision@k rata-rata
    """
    # Pilih pengguna dengan jumlah rating yang cukup
    users_with_enough_ratings = ratings_df['userId'].value_counts()[ratings_df['userId'].value_counts() >= 20].index

    # Batasi jumlah pengguna untuk evaluasi
    sample_users = np.random.choice(users_with_enough_ratings, min(100, len(users_with_enough_ratings)), replace=False)

    precision_at_k_scores = []

    for user_id in sample_users:
        # Dapatkan film yang disukai pengguna (rating >= 4)
        liked_movies = ratings_df[(ratings_df['userId'] == user_id) & (ratings_df['rating'] >= 4)]['movieId'].values

        if len(liked_movies) < 2:
            continue

        # Pisahkan satu film untuk dasar rekomendasi
        np.random.shuffle(liked_movies)
        base_movie = liked_movies[0]
        test_movies = liked_movies[1:]

        # Dapatkan rekomendasi berdasarkan film dasar
        if base_movie not in indices:
            continue

        recommendations = get_recommendations_content_based(base_movie, cosine_sim, movies_df, indices, movie_indices, n=k)
        recommended_ids = recommendations['movieId'].values

        # Hitung precision@k
        relevant_count = len(set(recommended_ids) & set(test_movies))
        precision_at_k = relevant_count / k

        precision_at_k_scores.append(precision_at_k)

    # Hitung rata-rata precision@k
    if precision_at_k_scores:
        return np.mean(precision_at_k_scores)
    else:
        return 0

# Evaluasi Collaborative Filtering
def evaluate_collaborative(model, test_dataset):
    """
    Evaluasi model Collaborative Filtering menggunakan RMSE

    Parameters:
    -----------
    model : tensorflow.keras.Model
        Model Collaborative Filtering
    test_dataset : tf.data.Dataset
        Dataset testing

    Returns:
    --------
    float
        Nilai RMSE
    """
    # Prediksi rating pada dataset testing
    y_pred = model.predict(test_dataset)

    # Dapatkan nilai sebenarnya
    y_true = np.concatenate([y for _, y in test_dataset], axis=0)

    # Hitung RMSE
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    return rmse

# Evaluasi kedua model
print("\nEvaluasi Model:")

# Evaluasi Content-Based Filtering
precision_at_k = evaluate_content_based(movies, filtered_ratings, indices, cosine_sim, movie_indices)
print(f"Content-Based Filtering - Precision@10: {precision_at_k:.4f}")

# Evaluasi Collaborative Filtering
rmse = evaluate_collaborative(model, test_dataset)
print(f"Collaborative Filtering - RMSE: {rmse:.4f}")

# Visualisasi hasil evaluasi
plt.figure(figsize=(12, 6))

# Precision@k untuk Content-Based Filtering
plt.subplot(1, 2, 1)
plt.bar(['Precision@10'], [precision_at_k], color='skyblue')
plt.title('Content-Based Filtering\nPrecision@10')
plt.ylim(0, 1)

# RMSE untuk Collaborative Filtering
plt.subplot(1, 2, 2)
plt.bar(['RMSE'], [rmse], color='salmon')
plt.title('Collaborative Filtering\nRMSE')
plt.ylim(0, 5)

plt.tight_layout()
plt.savefig('evaluation_metrics.png')
plt.close()

# Simpan model Collaborative Filtering
model.save('collaborative_filtering_model.keras')

print("\nProyek Sistem Rekomendasi Film Selesai!")

